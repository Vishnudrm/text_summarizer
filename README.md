# ğŸ¤– AI-Powered Text Summarizer

> Transform lengthy texts into concise, actionable summaries using DeepSeek-R1 AI model with beautiful web interfaces.

## âœ¨ Features

- **ğŸ¯ Smart Summarization**: Generates precise 3-bullet-point summaries
- **ğŸŒ Web Interface**: Interactive Gradio-based UI for easy text input
- **âš¡ REST API**: FastAPI backend for programmatic access
- **ğŸ”„ Real-time Processing**: Instant summarization without streaming delays
- **ğŸ“± Responsive Design**: Works seamlessly across all devices

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- Ollama installed and running locally
- DeepSeek-R1 model downloaded in Ollama

### Installation

1. **Clone or download the project files**

2. **Install dependencies**
   ```bash
   pip install gradio fastapi requests uvicorn
   ```

3. **Start Ollama service**
   ```bash
   ollama serve
   ```

4. **Pull DeepSeek-R1 model**
   ```bash
   ollama pull deepseek-r1
   ```

## ğŸ® Usage

### Web Interface (Gradio)

Run the interactive web application:

```bash
python gradio_app.py
```

- Open your browser to the displayed URL (typically `http://localhost:7860`)
- Paste your text in the input box
- Click "Submit" to get your summary
- View results in the output panel

### REST API (FastAPI)

Launch the API server:

```bash
uvicorn fastapi_app:app --reload
```

**API Endpoint**: `POST /summarize/`

**Example Request**:
```bash
curl -X POST "http://localhost:8000/summarize/" \
     -H "Content-Type: application/json" \
     -d '{"text": "Your long text here..."}'
```

**Example Response**:
```json
{
  "response": "â€¢ Key point 1\nâ€¢ Key point 2\nâ€¢ Key point 3"
}
```

## ğŸ“ Project Structure

```
ai-text-summarizer/
â”œâ”€â”€ gradio_app.py      # Web interface application
â”œâ”€â”€ fastapi_app.py     # REST API server
â””â”€â”€ README.md          # This file
```

## ğŸ› ï¸ Configuration

### Model Settings

Both applications use the DeepSeek-R1 model by default. To use a different model:

1. Change the `model` parameter in the payload
2. Ensure the model is available in your Ollama installation

### API Endpoint

The default Ollama endpoint is `http://localhost:11434/api/generate`. Update the `OLLAMA_URL` variable if your setup differs.

## ğŸ’¡ Use Cases

- **ğŸ“š Research**: Quickly digest academic papers and articles
- **ğŸ“° News**: Get key points from lengthy news articles
- **ğŸ“‹ Reports**: Extract essential information from business documents
- **ğŸ“– Content**: Summarize blog posts and web content
- **ğŸ“§ Communication**: Condense long emails and messages

## ğŸ”§ Troubleshooting

### Common Issues

**Connection Error**: Ensure Ollama is running on port 11434
```bash
ollama serve
```

**Model Not Found**: Verify DeepSeek-R1 is installed
```bash
ollama list
ollama pull deepseek-r1
```

**Port Conflicts**: Change ports in the launch commands
```python
# For Gradio
interface.launch(server_port=7861)

# For FastAPI
uvicorn fastapi_app:app --port 8001
```

## ğŸ¨ Customization

### Summary Format

Modify the prompt in the `summarize_text` function:

```python
"prompt": f"Summarize the following text in **5 key insights**:\n\n{text}"
```

### UI Styling

Customize the Gradio interface:

```python
interface = gr.Interface(
    # ... existing parameters ...
    theme="huggingface",
    css="custom.css"
)
```

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“ Support

Having issues? Check these resources:

- [Ollama Documentation](https://ollama.ai/docs)
- [Gradio Documentation](https://gradio.app/docs)
- [FastAPI Documentation](https://fastapi.tiangolo.com)

---

**Made with â¤ï¸ using DeepSeek-R1, Gradio, and FastAPI**